---
labal: single
title: "6.Decision Tree Regression"
---
## 의사 결정 나무 회귀 분석
 데이터들이 갖는 속성의 패턴을 예측 가능한 규칙들의 조합으로 구성하여 분류과제를 수행하는 지도 학습 알고리즘이다. 때문에 데이터를 지속적으로 분할하여 이후에 비슷한 범주의 관측치끼리 분류하고 이후에 새로운 관측치들의 값을 예측한다. 
 
 ## CART(Classification and Regression Trees)
 말 그대로 분류와 재귀가 모두 가능한 결정 나무 알고리즘이다. CART의 핵심적인 아이디어는 재귀적 분리와 가지치기이다. 재귀적인 분리는 각 데이터가 속한 파트에 균질성이 최대가 되도록 node로 분리하는 과정이고 가지치기는 이렇게 분리된 node가 과적합되는 것을 방지하기 위해 규칙들을 제거하는 과정이다. 이게 무슨 말인가? 재귀적 분리에서 **균질성(homogeneity)이란** 무슨 의미 일까?
 
 ## 불순도(Impurity)
 균질성과 반대되는 개념으로 의사결정 나무 node하나에 얼마나 많은 class들이 섞여있는지에 대한 값이다. 이때, 재귀적 분리 과정에서 균질성이 최대가 되도록 node를 분리하므로 불순도는 최소가 되는 방향으로 분리 된다. 
 다음은 불순도를 그림으로 나타낸것이다.
![image](https://github.com/minshik0705/minshik0705.github.io/assets/112872144/c36386d8-0396-4421-8bd8-921bc56da632)
이러한 불순도를 측정하는 방법은 Gini index와 Entropy측정의 과정을 통해 어떤 기준으로 데이터를 나눌지 정한다. 먼저 지니 점수부터 알아보자

![image](https://github.com/minshik0705/minshik0705.github.io/assets/112872144/54e288fd-2bc8-4d63-b1c4-cd59bde9213f)
서로 다른 class의 데이터가 얼마나 고르게 분포 되어있는지 측정하는 기법이며 위 그림에서 파란 O과 분홍 X를 개별적인 class로 보고 지니 점수를 측정한것으로 총 20개의 데이터 중 각 클래스가 10개씩이므로 위와 같은 계산이 나오는 것이다. 이때, 지니 점수가 가질수 있는 범위는 0~0.5이다

![image](https://github.com/minshik0705/minshik0705.github.io/assets/112872144/f7d8115f-3258-447f-9475-a56fc9802459)
이때 dataset을 다음과 같이 나누면 지니 점수의 계산은 위와 같이 0.253으로 줄어든다. 지니 점수가 측정한 불순도는 관측치가 얼마나 class가 섞여 있는지 측정한 값이므로 줄어든 불순도 만큼 정보량을 획득한다
여기서 획득한 정보량은 0.5-0.253=0.247이다. 이렇게 불순도가 최소가 되도록 정보량은 최대로 획득하도록 dataset을 나누고 각 구역이 leaf node가 되도록 의사결정 나무를 측정하면 된다. 
마찬가지로 엔트로피 점수를 통해 측정한 불순도도 지니 점수와 계산법은 다르지만 원리는 같다

![image](https://github.com/minshik0705/minshik0705.github.io/assets/112872144/d68fb715-d138-4514-8d2b-37b884777cfb)

## 관측치 예측
지금까지는 dataset을 어떻게 나눌까에 대한 이야기였다. 의사결정나무에 의해서 split이 되었으니 새로운 데이터가 입력이 되면 데이터가 속한 영역에 저절로 분류가 될것이다. 그러나, 그 새로운 데이터가 어떤 값을 가지게 될지 예측값을 무슨 기준으로 정하는가?
결론적으로 해당 영역의 관측값의 평균을 예측값 $\hat{y}$으로 정한다

![image](https://github.com/minshik0705/minshik0705.github.io/assets/112872144/276e5536-b9ca-43c3-90bc-5b717fdbc31e)
위의 경우 두 변수 X1, X2에 대해여 재귀적 분리를 진행하고 나뉜 각 영역의 관측치의 평균값을 초록 텍스트 창으로 표시한 것이다. 이를 의사결정 나무 구조로 살펴보면 다음과 같다. 

![image](https://github.com/minshik0705/minshik0705.github.io/assets/112872144/66370bbd-8465-419e-bca4-6087e7dff672)
tree의 형태로 나눌때는 당연히 yes no의 방향과 중간 노드의 비교연산의 방향이 같아야 한다. 여기서 새로운 데이터를 입력하면 나무의 논리를 타고 leaf node의 평균값을 예측값으로 설정한다
